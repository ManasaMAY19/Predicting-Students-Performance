# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
file_path = r"C:\Users\User\Downloads\StudentsPerformance.csv"
df = pd.read_csv(file_path)

# Display basic information about the dataset
print("Dataset shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())
print("\nData types:")
print(df.dtypes)
print("\nSummary statistics:")
print(df.describe())
print("\nMissing values:")
print(df.isnull().sum())

# Exploratory Data Analysis
plt.figure(figsize=(15, 10))

# Distribution of scores
plt.subplot(2, 3, 1)
sns.histplot(df['math score'], kde=True)
plt.title('Math Score Distribution')

plt.subplot(2, 3, 2)
sns.histplot(df['reading score'], kde=True)
plt.title('Reading Score Distribution')

plt.subplot(2, 3, 3)
sns.histplot(df['writing score'], kde=True)
plt.title('Writing Score Distribution')

# Correlation between scores
plt.subplot(2, 3, 4)
sns.heatmap(df[['math score', 'reading score', 'writing score']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Between Scores')

# Average scores by gender
plt.subplot(2, 3, 5)
sns.barplot(x='gender', y='math score', data=df)
plt.title('Math Scores by Gender')

plt.subplot(2, 3, 6)
sns.barplot(x='parental level of education', y='math score', data=df)
plt.title('Math Scores by Parental Education')
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

# Additional analysis
plt.figure(figsize=(15, 10))

# Test preparation impact
plt.subplot(2, 2, 1)
sns.boxplot(x='test preparation course', y='math score', data=df)
plt.title('Test Prep Impact on Math Scores')

# Lunch type impact
plt.subplot(2, 2, 2)
sns.boxplot(x='lunch', y='math score', data=df)
plt.title('Lunch Type Impact on Math Scores')

# Race/ethnicity impact
plt.subplot(2, 2, 3)
sns.boxplot(x='race/ethnicity', y='math score', data=df)
plt.title('Race/Ethnicity Impact on Math Scores')
plt.xticks(rotation=45, ha='right')

# Pairplot for all scores
plt.figure(figsize=(10, 8))
sns.pairplot(df[['math score', 'reading score', 'writing score', 'gender']], hue='gender')
plt.suptitle('Pairplot of Scores by Gender', y=1.02)
plt.show()

# Feature Engineering
# Convert categorical variables to numerical using one-hot encoding
X = df.drop('math score', axis=1)
y = df['math score']

# Define categorical features
categorical_features = ['gender', 'race/ethnicity', 'parental level of education', 
                        'lunch', 'test preparation course']
numeric_features = []  # No numeric features to preprocess in this dataset

# Create preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

# Build model pipelines
models = {
    'Linear Regression': Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', LinearRegression())
    ]),
    'Random Forest': Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
    ])
}

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train and evaluate models
results = {}
for name, model in models.items():
    # Train model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Evaluate model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {'MSE': mse, 'R2': r2}
    
    print(f"\n{name} Results:")
    print(f"Mean Squared Error: {mse:.2f}")
    print(f"RÂ² Score: {r2:.2f}")

# Feature importance for Random Forest
if 'Random Forest' in models:
    rf_model = models['Random Forest']
    feature_names = (rf_model.named_steps['preprocessor']
                    .transformers_[0][1]
                    .get_feature_names_out(categorical_features))
    
    importances = rf_model.named_steps['regressor'].feature_importances_
    
    # Plot feature importance
    plt.figure(figsize=(10, 6))
    indices = np.argsort(importances)[::-1]
    plt.title('Feature Importance for Math Score Prediction')
    plt.bar(range(len(importances)), importances[indices])
    plt.xticks(range(len(importances)), feature_names[indices], rotation=90)
    plt.tight_layout()
    plt.show()
